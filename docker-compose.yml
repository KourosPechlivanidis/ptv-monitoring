# build a network to allow services to talk to each other
networks:
  kafka-network:
    name: kafka-network
    driver: bridge

services:
# kafka broker on KRaft mode
  kafka:
    image: confluentinc/cp-kafka:latest
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      KAFKA_KRAFT_MODE: "true" 
      KAFKA_PROCESS_ROLES: controller,broker 
      KAFKA_NODE_ID: 1 
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093" 
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,HOST://0.0.0.0:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,HOST://localhost:9094
      KAFKA_LOG_DIRS: /var/lib/kafka/data 
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"  
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 
      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk" 
    volumes:
      - ./data:/var/lib/kafka/data 
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/9092"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
  # initialise topics, can only be ran after broker is healthy
  init-kafka:
    build: ./ingestion/kafka-init
    networks:
      - kafka-network
    depends_on:
      kafka:
        condition: service_healthy
  # run producer that puts GTFS real time updates into kafka topics
  producer:
    build: ./ingestion/producer
    networks:
      - kafka-network
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092 
      GTFS_API_KEY: ${GTFS_API_KEY}
  redis:
      image: redis:7
      container_name: redis
      ports:
        - "6379:6379"
      networks:
        - kafka-network
  # Spark structured streaming consumer to consume vehicle positions from kafka
  vehicle-positions-consumer:
    build: ./streaming
    networks:
      - kafka-network
    command: ["process_vehicle_positions.py"]
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      REDIS_HOST: redis
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY}
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_started
  # Spark structured streaming consumer to consume trip updates from kafka
  trip-updates-consumer:
    build: ./streaming
    networks:
      - kafka-network
    command: ["process_trip_updates.py"]
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      REDIS_HOST: redis
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY}
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_started
    